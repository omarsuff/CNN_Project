{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facae81b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow \n",
    "# Importing necessary library or function\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7195ecf",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7447082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary library or function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885b576",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58ada8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(caption):\n",
    "    \"\"\"Improved caption cleaning\"\"\"\n",
    "    # Convert to lowercase\n",
    "    caption = caption.lower()\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    caption = re.sub(r'[^a-z0-9\\s.,!?]', '', caption)\n",
    "    \n",
    "    # Normalize spaces\n",
    "    caption = re.sub(r'\\s+', ' ', caption)\n",
    "    \n",
    "    # Remove single characters\n",
    "    caption = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', caption)\n",
    "    \n",
    "    # Add start and end tokens\n",
    "    caption = 'startseq ' + caption.strip() + ' endseq'\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Vocabulary Construction\n",
    "def create_vocabulary(captions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    vocab = tokenizer.word_index\n",
    "    vocab_size = len(vocab) + 1  # +1 for padding token\n",
    "    return tokenizer, vocab, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8e954",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d526bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and organize data\n",
    "def load_caption_data(filename, image_dir):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    image_caption_dict = {}\n",
    "    for line in data:\n",
    "        img_name, caption = line.strip().split('\\t')\n",
    "        img_name = os.path.join(image_dir, img_name)\n",
    "        if img_name not in image_caption_dict:\n",
    "            image_caption_dict[img_name] = []\n",
    "        image_caption_dict[img_name].append(clean_caption(caption))\n",
    "    return image_caption_dict\n",
    "\n",
    "# CNN Image Feature Extraction\n",
    "def extract_image_features(image_paths,folder,img_size=(224, 224)):\n",
    "    # Use ResNet50 without top layers for feature extraction\n",
    "    print(\"Initializing ResNet50 model...\")\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*img_size, 3))\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "    print(\"Model initialized successfully.\")\n",
    "    \n",
    "    features = {}\n",
    "    total_images = len(image_paths)\n",
    "    print(f\"Starting feature extraction for {total_images} images...\")\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        # Print progress more frequently\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing image {i}/{total_images} ({(i/total_images)*100:.2f}%)\")\n",
    "            # Force output buffer to flush\n",
    "# Importing necessary library or function\n",
    "            import sys\n",
    "            sys.stdout.flush()\n",
    "        try:\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "            feature = model.predict(img_array, verbose=0)\n",
    "            # Reshape to a fixed size for consistent input to decoder\n",
    "            feature = np.reshape(feature, (feature.shape[0], -1, feature.shape[3]))\n",
    "            img_id = os.join(folder, os.path.basename(img_path))\n",
    "            features[img_id] = feature\n",
    "            # Print a clear success message every 100 images\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                print(f\"✓ Successfully processed {i} images\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    print(f\"Feature extraction completed. Processed {len(features)} images successfully.\")\n",
    "    return features\n",
    "\n",
    "# Data generator for training\n",
    "def data_generator(image_features, captions_dict, tokenizer, max_length, batch_size, vocab_size):\n",
    "    # Get all valid image IDs (those that have features)\n",
    "    valid_img_ids = [img_id for img_id in captions_dict.keys() if img_id in image_features]\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle the image IDs for each epoch\n",
    "        np.random.shuffle(valid_img_ids)\n",
    "        \n",
    "        X1, X2, y = [], [], []\n",
    "        count = 0\n",
    "        \n",
    "        for img_id in valid_img_ids:\n",
    "            feature = image_features[img_id]\n",
    "            captions = captions_dict[img_id]\n",
    "            \n",
    "            # Randomly select one caption for this image\n",
    "            caption = np.random.choice(captions)\n",
    "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "            \n",
    "            # Generate input-output pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq = seq[:i]\n",
    "                out_seq = seq[i]\n",
    "                \n",
    "                # Pad sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                # One-hot encode output word\n",
    "                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                \n",
    "                X1.append(feature[0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "                \n",
    "                count += 1\n",
    "                if count == batch_size:\n",
    "                    yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                    X1, X2, y = [], [], []\n",
    "                    count = 0\n",
    "        \n",
    "        if count > 0:\n",
    "            yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "\n",
    "# Modify model architecture parameters in create_model()\n",
    "def create_model(vocab_size, max_length, embedding_dim=512, units=512):\n",
    "    # Image feature input\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.3)(inputs1)  # Reduced dropout for better feature retention\n",
    "    fe2 = Dense(embedding_dim, activation='relu')(fe1)\n",
    "    \n",
    "    # Sequence input with improved embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.3)(se1)  # Reduced dropout\n",
    "    se3 = LSTM(units, return_sequences=True)(se2)  # Added return_sequences\n",
    "    se4 = LSTM(units)(se3)  # Added second LSTM layer\n",
    "    \n",
    "    # Enhanced decoder\n",
    "    decoder1 = Add()([fe2, se4])\n",
    "    decoder2 = Dense(units * 2, activation='relu')(decoder1)  # Wider dense layer\n",
    "    decoder3 = Dropout(0.3)(decoder2)  # Added dropout\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder3)\n",
    "    \n",
    "    # Model with modified optimizer\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Modify the generate_caption function to handle batch dimensions correctly\n",
    "def generate_caption(model, image_feature, tokenizer, max_length):\n",
    "    # Start with the start sequence token\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # Add batch dimension to image_feature if not present\n",
    "    if len(image_feature.shape) == 1:\n",
    "        image_feature = np.expand_dims(image_feature, axis=0)\n",
    "    \n",
    "    # Iterate until max length or end token\n",
    "    for i in range(max_length):\n",
    "        # Convert the current text to a sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pad the sequence\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        \n",
    "        # Predict the next word\n",
    "        yhat = model.predict([image_feature, sequence], verbose=0)\n",
    "        # Get the index with highest probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Convert the index to a word\n",
    "        word = None\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == yhat:\n",
    "                word = w\n",
    "                break\n",
    "        \n",
    "        # Stop if we can't find the word or reach the end token\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        \n",
    "        # Append the word to the current text\n",
    "        in_text += ' ' + word\n",
    "    \n",
    "    # Remove the start token\n",
    "    caption = in_text.replace('startseq', '')\n",
    "    \n",
    "    return caption.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bbcb9",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a0c6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved parameters for better model performance\n",
    "max_length = 38  # Increased to allow longer captions\n",
    "embedding_dim = 256  # Doubled for better word representation\n",
    "units = 256  # Doubled LSTM units for more complex patterns\n",
    "batch_size = 32  # Increased for better stability\n",
    "epochs = 20  # More epochs for better convergence\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_captions_file = 'train.txt'  # Adjust path as needed\n",
    "validation_captions_file = 'val.txt'  # Adjust path as needed\n",
    "\n",
    "# Load captions and organize by image\n",
    "train_captions = load_caption_data(train_captions_file,'train/train')\n",
    "validation_captions = load_caption_data(validation_captions_file,'val/val')\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_ids, val_ids = train_captions.keys(), validation_captions.keys()\n",
    "\n",
    "# Create dictionaries for each split\n",
    "train_data = {img_id: train_captions[img_id] for img_id in train_ids}\n",
    "val_data = {img_id: validation_captions[img_id] for img_id in val_ids}\n",
    "\n",
    "# Create vocabulary from training captions\n",
    "train_captions = [caption for captions in train_data.values() for caption in captions]\n",
    "tokenizer, vocab, vocab_size = create_vocabulary(train_captions)\n",
    "\n",
    "# get test_ids from the test/test folder\n",
    "test_ids = os.listdir('test/test')\n",
    "test_ids = [os.path.join('test/test', img_id) for img_id in test_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07911b",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e06bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint the extracted features\n",
    "train_image_features_file = 'train_image_features.npy'\n",
    "val_image_features_file = 'val_image_features.npy'\n",
    "test_image_features_file = 'test_image_features.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5b87b",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6588e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting image features...\n",
      "Initializing ResNet50 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745870102.508238    1749 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting image features...\")\n",
    "test_image_features = extract_image_features(test_ids,'test/test')\n",
    "print(\"Image features extracted!\")\n",
    "\n",
    "np.save(test_image_features_file, test_image_features)\n",
    "print(\"Test image features saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f356c",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d83bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting image features...\")\n",
    "val_image_features = extract_image_features(val_ids,'val/val')\n",
    "print(\"Image features extracted!\")\n",
    "\n",
    "np.save(val_image_features_file, val_image_features)\n",
    "print(\"Validation image features saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4f33f",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting image features...\")\n",
    "train_image_features = extract_image_features(train_ids,'train/train')\n",
    "print(\"Image features extracted!\")\n",
    "\n",
    "np.save(train_image_features_file, train_image_features)\n",
    "print(\"Training image features saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474466a1",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468892f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image features: 6472 images\n",
      "Validation image features: 809 images\n",
      "Test image features: 810 images\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image data\n",
    "train_image_features = np.load(train_image_features_file, allow_pickle=True).item()\n",
    "val_image_features = np.load(val_image_features_file, allow_pickle=True).item()\n",
    "test_image_features = np.load(test_image_features_file, allow_pickle=True).item()\n",
    "\n",
    "# add folder to image_features id without changing the original type\n",
    "train_image_features = {os.path.join('train/train', k): v for k, v in train_image_features.items()}\n",
    "val_image_features = {os.path.join('val/val', k): v for k, v in val_image_features.items()}\n",
    "test_image_features = {os.path.join('test/test', k): v for k, v in test_image_features.items()}\n",
    "\n",
    "# Check if the image features are loaded correctly\n",
    "print(f\"Train image features: {len(train_image_features)} images\")\n",
    "print(f\"Validation image features: {len(val_image_features)} images\")\n",
    "print(f\"Test image features: {len(test_image_features)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c052c4",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34384912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "\n",
      "Verifying data shapes...\n",
      "Image features shape: (64, 2048)\n",
      "Text sequence shape: (64, 38)\n",
      "Label shape: (64, 2692)\n",
      "\n",
      "Feature stats:\n",
      "Min: 0.0\n",
      "Max: 15.335591316223145\n",
      "Mean: 0.6120313405990601\n"
     ]
    }
   ],
   "source": [
    "def create_tf_dataset(image_features, captions_dict, tokenizer, max_length, batch_size, vocab_size):\n",
    "    def generator():\n",
    "        valid_img_ids = [img_id for img_id in captions_dict.keys() if img_id in image_features]\n",
    "        while True:\n",
    "            np.random.shuffle(valid_img_ids)\n",
    "            for img_id in valid_img_ids:\n",
    "                # Get image features and reshape correctly\n",
    "                feature = image_features[img_id]\n",
    "                # Convert from (1, 49, 2048) to (2048,) by taking first element and flattening correctly\n",
    "                feature = np.mean(feature[0], axis=0)  # Average across spatial dimensions\n",
    "                \n",
    "                captions = captions_dict[img_id]\n",
    "                caption = np.random.choice(captions)\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                \n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq = seq[:i]\n",
    "                    out_seq = seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                    out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    # Ensure feature shape is correct\n",
    "                    yield (feature, in_seq.astype(np.float32)), out_seq\n",
    "\n",
    "    # Define output signature\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(max_length,), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # Add batch and prefetch\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Now update the training code\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = create_tf_dataset(\n",
    "    train_image_features, train_data, \n",
    "    tokenizer, max_length, batch_size, vocab_size\n",
    ")\n",
    "val_dataset = create_tf_dataset(\n",
    "    val_image_features, val_data, \n",
    "    tokenizer, max_length, batch_size, vocab_size\n",
    ")\n",
    "\n",
    "print(\"\\nVerifying data shapes...\")\n",
    "for (img_features, text_seq), label in train_dataset.take(1):\n",
    "    print(f\"Image features shape: {img_features.shape}\")\n",
    "    print(f\"Text sequence shape: {text_seq.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")\n",
    "    # Additional debug info\n",
    "    print(f\"\\nFeature stats:\")\n",
    "    print(f\"Min: {tf.reduce_min(img_features)}\")\n",
    "    print(f\"Max: {tf.reduce_max(img_features)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(img_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69d213",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "Training samples: 1,015,749\n",
      "Validation samples: 126,503\n",
      "Steps per epoch: 15,871\n",
      "Validation steps: 1,976\n",
      "Batch size: 64\n",
      "Epochs: 30\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745723879.350810  801601 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2380 - loss: 4.3234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-04-27 06:27:50.235971: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 330792960 exceeds 10% of free system memory.\n",
      "2025-04-27 06:27:50.431907: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 251658240 exceeds 10% of free system memory.\n",
      "2025-04-27 06:27:50.485628: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 251658240 exceeds 10% of free system memory.\n",
      "2025-04-27 06:27:50.545264: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 251658240 exceeds 10% of free system memory.\n",
      "2025-04-27 06:27:50.601469: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 251658240 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 37ms/step - accuracy: 0.2380 - loss: 4.3233 - val_accuracy: 0.3117 - val_loss: 3.5882 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m15870/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3131 - loss: 3.4853"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 37ms/step - accuracy: 0.3131 - loss: 3.4853 - val_accuracy: 0.3244 - val_loss: 3.5397 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 37ms/step - accuracy: 0.3233 - loss: 3.3763 - val_accuracy: 0.3285 - val_loss: 3.5748 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 37ms/step - accuracy: 0.3277 - loss: 3.3449 - val_accuracy: 0.3313 - val_loss: 3.6282 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m15870/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3294 - loss: 3.3350\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 37ms/step - accuracy: 0.3294 - loss: 3.3350 - val_accuracy: 0.3275 - val_loss: 3.6507 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 38ms/step - accuracy: 0.3394 - loss: 3.2020 - val_accuracy: 0.3378 - val_loss: 3.5882 - learning_rate: 2.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m15871/15871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 38ms/step - accuracy: 0.3519 - loss: 3.0613 - val_accuracy: 0.3395 - val_loss: 3.5950 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length, embedding_dim, units)\n",
    "\n",
    "# Improved callbacks configuration\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"model_checkpoint.h5\",\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,  # Increased patience\n",
    "        min_delta=0.005,  # More sensitive improvement threshold\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,  # More aggressive LR reduction\n",
    "        patience=3,\n",
    "        min_lr=1e-6,  # Added minimum learning rate\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# First, let's calculate exact steps needed per epoch\n",
    "def calculate_training_parameters():\n",
    "    # Count total valid samples\n",
    "    train_samples = sum(\n",
    "        len(tokenizer.texts_to_sequences([caption])[0]) - 1\n",
    "        for img_id in train_data.keys()\n",
    "        if img_id in train_image_features\n",
    "        for caption in train_data[img_id]\n",
    "    )\n",
    "    \n",
    "    steps_per_epoch = train_samples // batch_size\n",
    "    \n",
    "    # Calculate validation steps\n",
    "    val_samples = sum(\n",
    "        len(tokenizer.texts_to_sequences([caption])[0]) - 1\n",
    "        for img_id in val_data.keys()\n",
    "        if img_id in val_image_features\n",
    "        for caption in val_data[img_id]\n",
    "    )\n",
    "    \n",
    "    val_steps = val_samples // batch_size\n",
    "    \n",
    "    print(\"\\nTraining Configuration:\")\n",
    "    print(f\"Training samples: {train_samples:,}\")\n",
    "    print(f\"Validation samples: {val_samples:,}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch:,}\")\n",
    "    print(f\"Validation steps: {val_steps:,}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    \n",
    "    return steps_per_epoch, val_steps\n",
    "\n",
    "# Get proper step counts\n",
    "steps_per_epoch, validation_steps = calculate_training_parameters()\n",
    "\n",
    "# Modified training configuration\n",
    "try:\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset.repeat(),  # Add repeat() to prevent dataset exhaustion\n",
    "        steps_per_epoch=steps_per_epoch,  # Set proper steps\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset.repeat(),\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted. Saving model...\")\n",
    "    model.save('model_interrupted.h5')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed with error: {str(e)}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552ca34",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Update the prediction function with proper feature handling\n",
    "def predict_captions_for_test_images(model, test_image_features, tokenizer, max_length):\n",
    "    predictions = {}\n",
    "    length = len(test_image_features)\n",
    "    \n",
    "    for i, (img_id, feature) in enumerate(test_image_features.items()):\n",
    "        try:\n",
    "            # Reshape feature to match model input (from (1, 49, 2048) to (1, 2048))\n",
    "            feature = np.mean(feature[0], axis=0)  # Average across spatial dimensions\n",
    "            feature = np.expand_dims(feature, axis=0)  # Add batch dimension\n",
    "            \n",
    "            # Generate caption\n",
    "            caption = generate_caption(model, feature, tokenizer, max_length)\n",
    "\n",
    "            # Remove the file location from the image ID 'test/test/' using substring\n",
    "            img_id = img_id.replace('test/test/', '')\n",
    "            predictions[img_id] = caption\n",
    "            \n",
    "            # Progress reporting\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processed {i}/{length} images ({(i/length)*100:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_id}: {str(e)}\")\n",
    "            predictions[img_id] = \"Error generating caption\"\n",
    "            continue\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fdf3e",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac03476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a custom layer scope\n",
    "# Importing necessary library or function\n",
    "import tensorflow as tf\n",
    "# Importing necessary library or function\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "def load_trained_model(model_path, vocab_size, max_length, embedding_dim=256, units=256):\n",
    "    \"\"\"Load trained model with proper custom objects\"\"\"\n",
    "    # First create a fresh model with same architecture\n",
    "    model = create_model(vocab_size, max_length, embedding_dim, units)\n",
    "    \n",
    "    try:\n",
    "        # Load weights only\n",
    "        model.load_weights(model_path)\n",
    "        print(\"Model weights loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        # If loading fails, create new model\n",
    "        print(\"Creating new model instead...\")\n",
    "        model = create_model(vocab_size, max_length, embedding_dim, units)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1bde9",
   "metadata": {},
   "source": [
    "# Code Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b076689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and generating predictions...\n",
      "Model weights loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745760083.545105 1026245 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/810 images (0.0%)\n",
      "Processed 10/810 images (1.2%)\n",
      "Processed 20/810 images (2.5%)\n",
      "Processed 30/810 images (3.7%)\n",
      "Processed 40/810 images (4.9%)\n",
      "Processed 50/810 images (6.2%)\n",
      "Processed 60/810 images (7.4%)\n",
      "Processed 70/810 images (8.6%)\n",
      "Processed 80/810 images (9.9%)\n",
      "Processed 90/810 images (11.1%)\n",
      "Processed 100/810 images (12.3%)\n",
      "Processed 110/810 images (13.6%)\n",
      "Processed 120/810 images (14.8%)\n",
      "Processed 130/810 images (16.0%)\n",
      "Processed 140/810 images (17.3%)\n",
      "Processed 150/810 images (18.5%)\n",
      "Processed 160/810 images (19.8%)\n",
      "Processed 170/810 images (21.0%)\n",
      "Processed 180/810 images (22.2%)\n",
      "Processed 190/810 images (23.5%)\n",
      "Processed 200/810 images (24.7%)\n",
      "Processed 210/810 images (25.9%)\n",
      "Processed 220/810 images (27.2%)\n",
      "Processed 230/810 images (28.4%)\n",
      "Processed 240/810 images (29.6%)\n",
      "Processed 250/810 images (30.9%)\n",
      "Processed 260/810 images (32.1%)\n",
      "Processed 270/810 images (33.3%)\n",
      "Processed 280/810 images (34.6%)\n",
      "Processed 290/810 images (35.8%)\n",
      "Processed 300/810 images (37.0%)\n",
      "Processed 310/810 images (38.3%)\n",
      "Processed 320/810 images (39.5%)\n",
      "Processed 330/810 images (40.7%)\n",
      "Processed 340/810 images (42.0%)\n",
      "Processed 350/810 images (43.2%)\n",
      "Processed 360/810 images (44.4%)\n",
      "Processed 370/810 images (45.7%)\n",
      "Processed 380/810 images (46.9%)\n",
      "Processed 390/810 images (48.1%)\n",
      "Processed 400/810 images (49.4%)\n",
      "Processed 410/810 images (50.6%)\n",
      "Processed 420/810 images (51.9%)\n",
      "Processed 430/810 images (53.1%)\n",
      "Processed 440/810 images (54.3%)\n",
      "Processed 450/810 images (55.6%)\n",
      "Processed 460/810 images (56.8%)\n",
      "Processed 470/810 images (58.0%)\n",
      "Processed 480/810 images (59.3%)\n",
      "Processed 490/810 images (60.5%)\n",
      "Processed 500/810 images (61.7%)\n",
      "Processed 510/810 images (63.0%)\n",
      "Processed 520/810 images (64.2%)\n",
      "Processed 530/810 images (65.4%)\n",
      "Processed 540/810 images (66.7%)\n",
      "Processed 550/810 images (67.9%)\n",
      "Processed 560/810 images (69.1%)\n",
      "Processed 570/810 images (70.4%)\n",
      "Processed 580/810 images (71.6%)\n",
      "Processed 590/810 images (72.8%)\n",
      "Processed 600/810 images (74.1%)\n",
      "Processed 610/810 images (75.3%)\n",
      "Processed 620/810 images (76.5%)\n",
      "Processed 630/810 images (77.8%)\n",
      "Processed 640/810 images (79.0%)\n",
      "Processed 650/810 images (80.2%)\n",
      "Processed 660/810 images (81.5%)\n",
      "Processed 670/810 images (82.7%)\n",
      "Processed 680/810 images (84.0%)\n",
      "Processed 690/810 images (85.2%)\n",
      "Processed 700/810 images (86.4%)\n",
      "Processed 710/810 images (87.7%)\n",
      "Processed 720/810 images (88.9%)\n",
      "Processed 730/810 images (90.1%)\n",
      "Processed 740/810 images (91.4%)\n",
      "Processed 750/810 images (92.6%)\n",
      "Processed 760/810 images (93.8%)\n",
      "Processed 770/810 images (95.1%)\n",
      "Processed 780/810 images (96.3%)\n",
      "Processed 790/810 images (97.5%)\n",
      "Processed 800/810 images (98.8%)\n",
      "Predictions saved to test_predictions2.csv\n",
      "\n",
      "Sample predictions:\n",
      "                    image_id                                       caption\n",
      "0  1002674143_1b742ab4b8.jpg    a man in red shirt is sitting on the grass\n",
      "1  1003163366_44323f5815.jpg  a man in black shirt is sitting on the grass\n",
      "2  1016887272_03199f49c4.jpg           a man in red shirt is climbing rock\n",
      "3  1022454332_6af2c1449a.jpg                a man is standing on the beach\n",
      "4   103106960_e8a41d64f8.jpg   a man in red shirt is jumping off the <unk>\n"
     ]
    }
   ],
   "source": [
    "# Test the prediction\n",
    "try:\n",
    "    print(\"Loading model and generating predictions...\")\n",
    "    model_path = 'model_checkpoint.h5'\n",
    "    model = load_trained_model(model_path, vocab_size, max_length, embedding_dim, units)\n",
    "    \n",
    "    test_predictions = predict_captions_for_test_images(\n",
    "        model, test_image_features, tokenizer, max_length\n",
    "    )\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_df = pd.DataFrame(test_predictions.items(), columns=['image_id', 'caption'])\n",
    "    predictions_df.to_csv('test_predictions2.csv', index=False)\n",
    "    print(\"Predictions saved to test_predictions2.csv\")\n",
    "    \n",
    "    # Display first few predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(predictions_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {str(e)}\")\n",
    "# Importing necessary library or function\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMLproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
